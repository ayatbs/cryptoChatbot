{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amJlFwFat3SZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential #type: ignore\n",
        "from tensorflow.keras.layers import LSTM, Dense #type: ignore\n",
        "from datetime import timedelta\n",
        "import plotly.express as px\n",
        "from itertools import cycle\n",
        "import json\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from datetime import datetime, timedelta\n",
        "import math\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import json\n",
        "\n",
        "def build_predictor(symbol, days_ahead):\n",
        "    # Download historical stock data for a given symbol from Yahoo Finance and reset the DataFrame index.\n",
        "    data_frame = yf.download(symbol)\n",
        "    data_frame.reset_index(inplace=True)\n",
        "    price_data = data_frame[['Date', 'Close']]\n",
        "\n",
        "    # Get the current date and set the end date for the data filtering\n",
        "    current_date = datetime.now().date()\n",
        "    end_date = current_date + timedelta(days=days_ahead)\n",
        "\n",
        "    # Filter data to include records between February 15, 2020, and the specified end date\n",
        "    price_data = price_data[(price_data['Date'] > '2020-02-15') & (price_data['Date'] <= end_date.strftime('%Y-%m-%d'))]\n",
        "    processed_data = price_data.copy()\n",
        "\n",
        "    # Scale the closing stock prices to a range between 0 and 1 for normalization.\n",
        "    final_date = processed_data['Date'].iloc[-1]  # Store the last date for future use.\n",
        "    processed_data.drop('Date', axis=1, inplace=True)\n",
        "    price_scaler = MinMaxScaler((0, 1))\n",
        "    processed_data = price_scaler.fit_transform(np.array(processed_data).reshape(-1, 1))\n",
        "\n",
        "    # Split the normalized data into 60% training and 40% testing datasets.\n",
        "    split_ratio = int(len(processed_data) * 0.60)\n",
        "    train_set, test_set = processed_data[:split_ratio, :], processed_data[split_ratio:, :]\n",
        "\n",
        "    # Define a function to create input features and target variables arrays for model training/testing.\n",
        "    def generate_dataset(data, steps=1):\n",
        "        X, Y = [], []\n",
        "        for i in range(len(data) - steps - 1):\n",
        "            X.append(data[i:(i + steps), 0])  # Sequence of 'steps' observations as input features.\n",
        "            Y.append(data[i + steps, 0])      # Next observation as the target variable.\n",
        "        return np.array(X), np.array(Y)\n",
        "\n",
        "    # Create datasets for training and testing the model.\n",
        "    steps = 15  # Number of past days the model looks at to make a prediction.\n",
        "    features_train, target_train = generate_dataset(train_set, steps)\n",
        "    features_test, target_test = generate_dataset(test_set, steps)\n",
        "\n",
        "    # Reshape input arrays to fit LSTM model expected input [samples, time steps, features].\n",
        "    features_train = features_train.reshape(features_train.shape[0], features_train.shape[1], 1)\n",
        "    features_test = features_test.reshape(features_test.shape[0], features_test.shape[1], 1)\n",
        "\n",
        "    # Initialize, compile and train the LSTM network model.\n",
        "    lstm_model = Sequential()\n",
        "    lstm_model.add(LSTM(10, input_shape=(steps, 1), activation='relu'))\n",
        "    lstm_model.add(Dense(1))\n",
        "    lstm_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse', 'mae'])\n",
        "    lstm_model.fit(features_train, target_train, epochs=200, batch_size=32, verbose=1, validation_data=(features_test, target_test))\n",
        "\n",
        "    # Use the trained model to make predictions on the training and testing datasets.\n",
        "    train_predictions = lstm_model.predict(features_train)\n",
        "    test_predictions = lstm_model.predict(features_test)\n",
        "\n",
        "    # Convert the predictions back to the original scale using the previously defined MinMaxScaler.\n",
        "    train_predictions = price_scaler.inverse_transform(train_predictions)\n",
        "    test_predictions = price_scaler.inverse_transform(test_predictions)\n",
        "    real_train = price_scaler.inverse_transform(target_train.reshape(-1,1))\n",
        "    real_test = price_scaler.inverse_transform(target_test.reshape(-1,1))\n",
        "\n",
        "    # Calculate and print evaluation metrics for the model performance.\n",
        "    print(\"Training RMSE:\", math.sqrt(mean_squared_error(real_train, train_predictions)))\n",
        "    print(\"Training R2 score:\", r2_score(real_train, train_predictions))\n",
        "    print(\"Testing RMSE:\", math.sqrt(mean_squared_error(real_test, test_predictions)))\n",
        "    print(\"Testing R2 score:\", r2_score(real_test, test_predictions))\n",
        "\n",
        "    # Prepare input for future price prediction based on the last 'steps' observations from the test set.\n",
        "    future_input = test_set[-steps:].reshape(1, -1)\n",
        "    future_temp = list(future_input[0])\n",
        "\n",
        "    # Predict future stock prices for the specified number of days ahead.\n",
        "    future_predictions = {}\n",
        "    for i in range(days_ahead):\n",
        "        future_input = np.array(future_temp[-steps:]).reshape(1, -1, 1)\n",
        "        predicted_value = lstm_model.predict(future_input, verbose=0)\n",
        "        predicted_price = price_scaler.inverse_transform(predicted_value)\n",
        "        future_temp.extend(predicted_value[0])\n",
        "\n",
        "        future_date = (final_date + timedelta(days=i + 1)).strftime('%Y-%m-%d')\n",
        "        future_predictions[future_date] = float(predicted_price[0][0])\n",
        "\n",
        "    # Format the filename based on the stock symbol.\n",
        "    filename = 'predictions' + symbol.split('-')[0] + '.json'\n",
        "\n",
        "    # Save the future predictions to a JSON file for further use.\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(future_predictions, file, indent=4)\n",
        "\n",
        "    return future_predictions"
      ],
      "metadata": {
        "id": "yEKh0bYfuX5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "build_predictor(\"BTC-USD\", 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DShRtovGutR5",
        "outputId": "4e76e3da-99fa-4f65-d190-f4f3fb6f4877"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "29/29 [==============================] - 5s 26ms/step - loss: 0.2386 - mse: 0.2386 - mae: 0.3972 - val_loss: 0.1378 - val_mse: 0.1378 - val_mae: 0.3144\n",
            "Epoch 2/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.1145 - mse: 0.1145 - mae: 0.2665 - val_loss: 0.0573 - val_mse: 0.0573 - val_mae: 0.1845\n",
            "Epoch 3/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.0424 - mse: 0.0424 - mae: 0.1652 - val_loss: 0.0143 - val_mse: 0.0143 - val_mae: 0.0846\n",
            "Epoch 4/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 0.0083 - mse: 0.0083 - mae: 0.0735 - val_loss: 0.0012 - val_mse: 0.0012 - val_mae: 0.0245\n",
            "Epoch 5/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.0023 - mse: 0.0023 - mae: 0.0361 - val_loss: 0.0010 - val_mse: 0.0010 - val_mae: 0.0206\n",
            "Epoch 6/200\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 0.0019 - mse: 0.0019 - mae: 0.0312 - val_loss: 0.0011 - val_mse: 0.0011 - val_mae: 0.0212\n",
            "Epoch 7/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.0019 - mse: 0.0019 - mae: 0.0303 - val_loss: 0.0010 - val_mse: 0.0010 - val_mae: 0.0207\n",
            "Epoch 8/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.0018 - mse: 0.0018 - mae: 0.0300 - val_loss: 0.0010 - val_mse: 0.0010 - val_mae: 0.0207\n",
            "Epoch 9/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.0018 - mse: 0.0018 - mae: 0.0299 - val_loss: 0.0010 - val_mse: 0.0010 - val_mae: 0.0209\n",
            "Epoch 10/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.0018 - mse: 0.0018 - mae: 0.0294 - val_loss: 9.7201e-04 - val_mse: 9.7201e-04 - val_mae: 0.0206\n",
            "Epoch 11/200\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 0.0018 - mse: 0.0018 - mae: 0.0294 - val_loss: 9.5718e-04 - val_mse: 9.5718e-04 - val_mae: 0.0202\n",
            "Epoch 12/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0287 - val_loss: 9.3619e-04 - val_mse: 9.3619e-04 - val_mae: 0.0200\n",
            "Epoch 13/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0285 - val_loss: 9.2732e-04 - val_mse: 9.2732e-04 - val_mae: 0.0196\n",
            "Epoch 14/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.0017 - mse: 0.0017 - mae: 0.0279 - val_loss: 9.0608e-04 - val_mse: 9.0608e-04 - val_mae: 0.0196\n",
            "Epoch 15/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.0016 - mse: 0.0016 - mae: 0.0271 - val_loss: 8.7461e-04 - val_mse: 8.7461e-04 - val_mae: 0.0196\n",
            "Epoch 16/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.0015 - mse: 0.0015 - mae: 0.0270 - val_loss: 8.8289e-04 - val_mse: 8.8289e-04 - val_mae: 0.0192\n",
            "Epoch 17/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.0014 - mse: 0.0014 - mae: 0.0267 - val_loss: 8.4005e-04 - val_mse: 8.4005e-04 - val_mae: 0.0209\n",
            "Epoch 18/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0252 - val_loss: 7.8463e-04 - val_mse: 7.8463e-04 - val_mae: 0.0204\n",
            "Epoch 19/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.0013 - mse: 0.0013 - mae: 0.0251 - val_loss: 7.0963e-04 - val_mse: 7.0963e-04 - val_mae: 0.0186\n",
            "Epoch 20/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0243 - val_loss: 7.4853e-04 - val_mse: 7.4853e-04 - val_mae: 0.0210\n",
            "Epoch 21/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0241 - val_loss: 7.2282e-04 - val_mse: 7.2282e-04 - val_mae: 0.0207\n",
            "Epoch 22/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.0012 - mse: 0.0012 - mae: 0.0249 - val_loss: 6.9085e-04 - val_mse: 6.9085e-04 - val_mae: 0.0194\n",
            "Epoch 23/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0238 - val_loss: 6.7738e-04 - val_mse: 6.7738e-04 - val_mae: 0.0176\n",
            "Epoch 24/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0234 - val_loss: 7.1377e-04 - val_mse: 7.1377e-04 - val_mae: 0.0212\n",
            "Epoch 25/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0235 - val_loss: 6.4730e-04 - val_mse: 6.4730e-04 - val_mae: 0.0180\n",
            "Epoch 26/200\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0234 - val_loss: 6.6516e-04 - val_mse: 6.6516e-04 - val_mae: 0.0194\n",
            "Epoch 27/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 9.9166e-04 - mse: 9.9166e-04 - mae: 0.0223 - val_loss: 5.9402e-04 - val_mse: 5.9402e-04 - val_mae: 0.0171\n",
            "Epoch 28/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 0.0010 - mse: 0.0010 - mae: 0.0226 - val_loss: 6.0184e-04 - val_mse: 6.0184e-04 - val_mae: 0.0182\n",
            "Epoch 29/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 9.8701e-04 - mse: 9.8701e-04 - mae: 0.0224 - val_loss: 6.4232e-04 - val_mse: 6.4232e-04 - val_mae: 0.0182\n",
            "Epoch 30/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 9.7125e-04 - mse: 9.7125e-04 - mae: 0.0222 - val_loss: 6.4925e-04 - val_mse: 6.4925e-04 - val_mae: 0.0200\n",
            "Epoch 31/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 9.7604e-04 - mse: 9.7604e-04 - mae: 0.0222 - val_loss: 6.2425e-04 - val_mse: 6.2425e-04 - val_mae: 0.0195\n",
            "Epoch 32/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 9.9613e-04 - mse: 9.9613e-04 - mae: 0.0224 - val_loss: 5.8978e-04 - val_mse: 5.8978e-04 - val_mae: 0.0162\n",
            "Epoch 33/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 9.4959e-04 - mse: 9.4959e-04 - mae: 0.0220 - val_loss: 6.3792e-04 - val_mse: 6.3792e-04 - val_mae: 0.0166\n",
            "Epoch 34/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 9.4620e-04 - mse: 9.4620e-04 - mae: 0.0218 - val_loss: 5.7183e-04 - val_mse: 5.7183e-04 - val_mae: 0.0163\n",
            "Epoch 35/200\n",
            "29/29 [==============================] - 0s 15ms/step - loss: 9.3125e-04 - mse: 9.3125e-04 - mae: 0.0218 - val_loss: 6.6377e-04 - val_mse: 6.6377e-04 - val_mae: 0.0166\n",
            "Epoch 36/200\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 0.0011 - mse: 0.0011 - mae: 0.0241 - val_loss: 7.6519e-04 - val_mse: 7.6519e-04 - val_mae: 0.0227\n",
            "Epoch 37/200\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 9.5197e-04 - mse: 9.5197e-04 - mae: 0.0222 - val_loss: 5.5013e-04 - val_mse: 5.5013e-04 - val_mae: 0.0164\n",
            "Epoch 38/200\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 9.2218e-04 - mse: 9.2218e-04 - mae: 0.0215 - val_loss: 5.5138e-04 - val_mse: 5.5138e-04 - val_mae: 0.0172\n",
            "Epoch 39/200\n",
            "29/29 [==============================] - 1s 18ms/step - loss: 8.9790e-04 - mse: 8.9790e-04 - mae: 0.0213 - val_loss: 5.3365e-04 - val_mse: 5.3365e-04 - val_mae: 0.0168\n",
            "Epoch 40/200\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 8.9408e-04 - mse: 8.9408e-04 - mae: 0.0214 - val_loss: 6.0330e-04 - val_mse: 6.0330e-04 - val_mae: 0.0193\n",
            "Epoch 41/200\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 9.4312e-04 - mse: 9.4312e-04 - mae: 0.0222 - val_loss: 7.0478e-04 - val_mse: 7.0478e-04 - val_mae: 0.0218\n",
            "Epoch 42/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 8.8751e-04 - mse: 8.8751e-04 - mae: 0.0212 - val_loss: 5.2498e-04 - val_mse: 5.2498e-04 - val_mae: 0.0169\n",
            "Epoch 43/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 8.5475e-04 - mse: 8.5475e-04 - mae: 0.0207 - val_loss: 5.2637e-04 - val_mse: 5.2637e-04 - val_mae: 0.0167\n",
            "Epoch 44/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 8.7114e-04 - mse: 8.7114e-04 - mae: 0.0210 - val_loss: 5.1099e-04 - val_mse: 5.1099e-04 - val_mae: 0.0153\n",
            "Epoch 45/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 8.5304e-04 - mse: 8.5304e-04 - mae: 0.0209 - val_loss: 5.2929e-04 - val_mse: 5.2929e-04 - val_mae: 0.0160\n",
            "Epoch 46/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 8.5099e-04 - mse: 8.5099e-04 - mae: 0.0206 - val_loss: 5.3624e-04 - val_mse: 5.3624e-04 - val_mae: 0.0176\n",
            "Epoch 47/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 8.4358e-04 - mse: 8.4358e-04 - mae: 0.0206 - val_loss: 5.2218e-04 - val_mse: 5.2218e-04 - val_mae: 0.0171\n",
            "Epoch 48/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 8.2864e-04 - mse: 8.2864e-04 - mae: 0.0204 - val_loss: 4.9852e-04 - val_mse: 4.9852e-04 - val_mae: 0.0157\n",
            "Epoch 49/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 8.4158e-04 - mse: 8.4158e-04 - mae: 0.0207 - val_loss: 4.8740e-04 - val_mse: 4.8740e-04 - val_mae: 0.0153\n",
            "Epoch 50/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 8.1763e-04 - mse: 8.1763e-04 - mae: 0.0202 - val_loss: 4.8075e-04 - val_mse: 4.8075e-04 - val_mae: 0.0154\n",
            "Epoch 51/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 8.1365e-04 - mse: 8.1365e-04 - mae: 0.0200 - val_loss: 5.0682e-04 - val_mse: 5.0682e-04 - val_mae: 0.0169\n",
            "Epoch 52/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 8.4014e-04 - mse: 8.4014e-04 - mae: 0.0206 - val_loss: 5.0070e-04 - val_mse: 5.0070e-04 - val_mae: 0.0166\n",
            "Epoch 53/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 8.0736e-04 - mse: 8.0736e-04 - mae: 0.0200 - val_loss: 4.8748e-04 - val_mse: 4.8748e-04 - val_mae: 0.0161\n",
            "Epoch 54/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 8.4946e-04 - mse: 8.4946e-04 - mae: 0.0208 - val_loss: 4.8758e-04 - val_mse: 4.8758e-04 - val_mae: 0.0145\n",
            "Epoch 55/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 7.9699e-04 - mse: 7.9699e-04 - mae: 0.0200 - val_loss: 4.6886e-04 - val_mse: 4.6886e-04 - val_mae: 0.0144\n",
            "Epoch 56/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 8.1667e-04 - mse: 8.1667e-04 - mae: 0.0204 - val_loss: 4.5726e-04 - val_mse: 4.5726e-04 - val_mae: 0.0144\n",
            "Epoch 57/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 8.2234e-04 - mse: 8.2234e-04 - mae: 0.0204 - val_loss: 4.8448e-04 - val_mse: 4.8448e-04 - val_mae: 0.0142\n",
            "Epoch 58/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 7.9828e-04 - mse: 7.9828e-04 - mae: 0.0200 - val_loss: 4.7065e-04 - val_mse: 4.7065e-04 - val_mae: 0.0141\n",
            "Epoch 59/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 8.0909e-04 - mse: 8.0909e-04 - mae: 0.0202 - val_loss: 4.7617e-04 - val_mse: 4.7617e-04 - val_mae: 0.0162\n",
            "Epoch 60/200\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 7.8241e-04 - mse: 7.8241e-04 - mae: 0.0197 - val_loss: 4.6294e-04 - val_mse: 4.6294e-04 - val_mae: 0.0141\n",
            "Epoch 61/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 8.0465e-04 - mse: 8.0465e-04 - mae: 0.0203 - val_loss: 4.5047e-04 - val_mse: 4.5047e-04 - val_mae: 0.0152\n",
            "Epoch 62/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 7.7656e-04 - mse: 7.7656e-04 - mae: 0.0197 - val_loss: 4.7552e-04 - val_mse: 4.7552e-04 - val_mae: 0.0163\n",
            "Epoch 63/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 7.8129e-04 - mse: 7.8129e-04 - mae: 0.0198 - val_loss: 4.5336e-04 - val_mse: 4.5336e-04 - val_mae: 0.0139\n",
            "Epoch 64/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 7.7962e-04 - mse: 7.7962e-04 - mae: 0.0198 - val_loss: 4.5102e-04 - val_mse: 4.5102e-04 - val_mae: 0.0153\n",
            "Epoch 65/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 7.9447e-04 - mse: 7.9447e-04 - mae: 0.0200 - val_loss: 4.9817e-04 - val_mse: 4.9817e-04 - val_mae: 0.0172\n",
            "Epoch 66/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 7.7634e-04 - mse: 7.7634e-04 - mae: 0.0198 - val_loss: 4.5095e-04 - val_mse: 4.5095e-04 - val_mae: 0.0155\n",
            "Epoch 67/200\n",
            "29/29 [==============================] - 0s 8ms/step - loss: 7.6767e-04 - mse: 7.6767e-04 - mae: 0.0196 - val_loss: 4.6817e-04 - val_mse: 4.6817e-04 - val_mae: 0.0138\n",
            "Epoch 68/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 7.4827e-04 - mse: 7.4827e-04 - mae: 0.0194 - val_loss: 4.6879e-04 - val_mse: 4.6879e-04 - val_mae: 0.0138\n",
            "Epoch 69/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 8.1367e-04 - mse: 8.1367e-04 - mae: 0.0202 - val_loss: 5.2589e-04 - val_mse: 5.2589e-04 - val_mae: 0.0145\n",
            "Epoch 70/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 7.5580e-04 - mse: 7.5580e-04 - mae: 0.0194 - val_loss: 4.2009e-04 - val_mse: 4.2009e-04 - val_mae: 0.0141\n",
            "Epoch 71/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 7.3550e-04 - mse: 7.3550e-04 - mae: 0.0190 - val_loss: 4.8414e-04 - val_mse: 4.8414e-04 - val_mae: 0.0139\n",
            "Epoch 72/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 7.6782e-04 - mse: 7.6782e-04 - mae: 0.0200 - val_loss: 4.2160e-04 - val_mse: 4.2160e-04 - val_mae: 0.0144\n",
            "Epoch 73/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 7.5290e-04 - mse: 7.5290e-04 - mae: 0.0194 - val_loss: 4.5579e-04 - val_mse: 4.5579e-04 - val_mae: 0.0159\n",
            "Epoch 74/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 7.2247e-04 - mse: 7.2247e-04 - mae: 0.0189 - val_loss: 4.1094e-04 - val_mse: 4.1094e-04 - val_mae: 0.0133\n",
            "Epoch 75/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 7.3274e-04 - mse: 7.3274e-04 - mae: 0.0191 - val_loss: 4.1276e-04 - val_mse: 4.1276e-04 - val_mae: 0.0139\n",
            "Epoch 76/200\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 7.3642e-04 - mse: 7.3642e-04 - mae: 0.0193 - val_loss: 5.1886e-04 - val_mse: 5.1886e-04 - val_mae: 0.0180\n",
            "Epoch 77/200\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 7.3520e-04 - mse: 7.3520e-04 - mae: 0.0193 - val_loss: 4.0210e-04 - val_mse: 4.0210e-04 - val_mae: 0.0138\n",
            "Epoch 78/200\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 7.3445e-04 - mse: 7.3445e-04 - mae: 0.0194 - val_loss: 3.9906e-04 - val_mse: 3.9906e-04 - val_mae: 0.0134\n",
            "Epoch 79/200\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 7.6088e-04 - mse: 7.6088e-04 - mae: 0.0195 - val_loss: 4.1224e-04 - val_mse: 4.1224e-04 - val_mae: 0.0144\n",
            "Epoch 80/200\n",
            "29/29 [==============================] - 0s 15ms/step - loss: 7.1015e-04 - mse: 7.1015e-04 - mae: 0.0187 - val_loss: 4.0216e-04 - val_mse: 4.0216e-04 - val_mae: 0.0140\n",
            "Epoch 81/200\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 6.9243e-04 - mse: 6.9243e-04 - mae: 0.0184 - val_loss: 5.2629e-04 - val_mse: 5.2629e-04 - val_mae: 0.0181\n",
            "Epoch 82/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 7.4875e-04 - mse: 7.4875e-04 - mae: 0.0197 - val_loss: 4.1121e-04 - val_mse: 4.1121e-04 - val_mae: 0.0140\n",
            "Epoch 83/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 7.0754e-04 - mse: 7.0754e-04 - mae: 0.0189 - val_loss: 3.8611e-04 - val_mse: 3.8611e-04 - val_mae: 0.0133\n",
            "Epoch 84/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 6.9667e-04 - mse: 6.9667e-04 - mae: 0.0185 - val_loss: 3.7633e-04 - val_mse: 3.7633e-04 - val_mae: 0.0127\n",
            "Epoch 85/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.9181e-04 - mse: 6.9181e-04 - mae: 0.0185 - val_loss: 4.0168e-04 - val_mse: 4.0168e-04 - val_mae: 0.0129\n",
            "Epoch 86/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.8258e-04 - mse: 6.8258e-04 - mae: 0.0184 - val_loss: 3.8469e-04 - val_mse: 3.8469e-04 - val_mae: 0.0134\n",
            "Epoch 87/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.9197e-04 - mse: 6.9197e-04 - mae: 0.0185 - val_loss: 3.8446e-04 - val_mse: 3.8446e-04 - val_mae: 0.0127\n",
            "Epoch 88/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.7967e-04 - mse: 6.7967e-04 - mae: 0.0183 - val_loss: 3.7150e-04 - val_mse: 3.7150e-04 - val_mae: 0.0126\n",
            "Epoch 89/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 7.0069e-04 - mse: 7.0069e-04 - mae: 0.0188 - val_loss: 3.9202e-04 - val_mse: 3.9202e-04 - val_mae: 0.0138\n",
            "Epoch 90/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 6.7839e-04 - mse: 6.7839e-04 - mae: 0.0183 - val_loss: 4.3506e-04 - val_mse: 4.3506e-04 - val_mae: 0.0133\n",
            "Epoch 91/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.8765e-04 - mse: 6.8765e-04 - mae: 0.0184 - val_loss: 3.7947e-04 - val_mse: 3.7947e-04 - val_mae: 0.0126\n",
            "Epoch 92/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.7716e-04 - mse: 6.7716e-04 - mae: 0.0182 - val_loss: 3.7693e-04 - val_mse: 3.7693e-04 - val_mae: 0.0125\n",
            "Epoch 93/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.6458e-04 - mse: 6.6458e-04 - mae: 0.0180 - val_loss: 3.7094e-04 - val_mse: 3.7094e-04 - val_mae: 0.0131\n",
            "Epoch 94/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.7087e-04 - mse: 6.7087e-04 - mae: 0.0181 - val_loss: 3.6744e-04 - val_mse: 3.6744e-04 - val_mae: 0.0124\n",
            "Epoch 95/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.8451e-04 - mse: 6.8451e-04 - mae: 0.0187 - val_loss: 5.5303e-04 - val_mse: 5.5303e-04 - val_mae: 0.0192\n",
            "Epoch 96/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 7.3953e-04 - mse: 7.3953e-04 - mae: 0.0197 - val_loss: 4.0515e-04 - val_mse: 4.0515e-04 - val_mae: 0.0128\n",
            "Epoch 97/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 6.6860e-04 - mse: 6.6860e-04 - mae: 0.0182 - val_loss: 3.7944e-04 - val_mse: 3.7944e-04 - val_mae: 0.0140\n",
            "Epoch 98/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 6.6782e-04 - mse: 6.6782e-04 - mae: 0.0181 - val_loss: 3.5814e-04 - val_mse: 3.5814e-04 - val_mae: 0.0129\n",
            "Epoch 99/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 6.5916e-04 - mse: 6.5916e-04 - mae: 0.0181 - val_loss: 3.8426e-04 - val_mse: 3.8426e-04 - val_mae: 0.0143\n",
            "Epoch 100/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.7954e-04 - mse: 6.7954e-04 - mae: 0.0185 - val_loss: 3.6195e-04 - val_mse: 3.6195e-04 - val_mae: 0.0122\n",
            "Epoch 101/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 6.7125e-04 - mse: 6.7125e-04 - mae: 0.0180 - val_loss: 3.4933e-04 - val_mse: 3.4933e-04 - val_mae: 0.0122\n",
            "Epoch 102/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 6.6490e-04 - mse: 6.6490e-04 - mae: 0.0181 - val_loss: 3.5628e-04 - val_mse: 3.5628e-04 - val_mae: 0.0122\n",
            "Epoch 103/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 6.5598e-04 - mse: 6.5598e-04 - mae: 0.0180 - val_loss: 3.8141e-04 - val_mse: 3.8141e-04 - val_mae: 0.0124\n",
            "Epoch 104/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.4071e-04 - mse: 6.4071e-04 - mae: 0.0178 - val_loss: 3.4222e-04 - val_mse: 3.4222e-04 - val_mae: 0.0124\n",
            "Epoch 105/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.3110e-04 - mse: 6.3110e-04 - mae: 0.0176 - val_loss: 3.3407e-04 - val_mse: 3.3407e-04 - val_mae: 0.0120\n",
            "Epoch 106/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.1669e-04 - mse: 6.1669e-04 - mae: 0.0173 - val_loss: 3.4858e-04 - val_mse: 3.4858e-04 - val_mae: 0.0130\n",
            "Epoch 107/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.2567e-04 - mse: 6.2567e-04 - mae: 0.0175 - val_loss: 3.3607e-04 - val_mse: 3.3607e-04 - val_mae: 0.0123\n",
            "Epoch 108/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.2811e-04 - mse: 6.2811e-04 - mae: 0.0174 - val_loss: 4.5112e-04 - val_mse: 4.5112e-04 - val_mae: 0.0140\n",
            "Epoch 109/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 6.2959e-04 - mse: 6.2959e-04 - mae: 0.0178 - val_loss: 3.3436e-04 - val_mse: 3.3436e-04 - val_mae: 0.0125\n",
            "Epoch 110/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 6.2121e-04 - mse: 6.2121e-04 - mae: 0.0174 - val_loss: 3.7478e-04 - val_mse: 3.7478e-04 - val_mae: 0.0123\n",
            "Epoch 111/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.3781e-04 - mse: 6.3781e-04 - mae: 0.0179 - val_loss: 3.9449e-04 - val_mse: 3.9449e-04 - val_mae: 0.0150\n",
            "Epoch 112/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.0972e-04 - mse: 6.0972e-04 - mae: 0.0173 - val_loss: 3.4249e-04 - val_mse: 3.4249e-04 - val_mae: 0.0128\n",
            "Epoch 113/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 6.1462e-04 - mse: 6.1462e-04 - mae: 0.0176 - val_loss: 3.8843e-04 - val_mse: 3.8843e-04 - val_mae: 0.0126\n",
            "Epoch 114/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 6.1859e-04 - mse: 6.1859e-04 - mae: 0.0175 - val_loss: 3.3364e-04 - val_mse: 3.3364e-04 - val_mae: 0.0126\n",
            "Epoch 115/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.5291e-04 - mse: 6.5291e-04 - mae: 0.0180 - val_loss: 3.1983e-04 - val_mse: 3.1983e-04 - val_mae: 0.0119\n",
            "Epoch 116/200\n",
            "29/29 [==============================] - 0s 15ms/step - loss: 6.0885e-04 - mse: 6.0885e-04 - mae: 0.0172 - val_loss: 3.2051e-04 - val_mse: 3.2051e-04 - val_mae: 0.0116\n",
            "Epoch 117/200\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 5.9775e-04 - mse: 5.9775e-04 - mae: 0.0169 - val_loss: 3.2341e-04 - val_mse: 3.2341e-04 - val_mae: 0.0123\n",
            "Epoch 118/200\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 5.9898e-04 - mse: 5.9898e-04 - mae: 0.0169 - val_loss: 3.2988e-04 - val_mse: 3.2988e-04 - val_mae: 0.0126\n",
            "Epoch 119/200\n",
            "29/29 [==============================] - 0s 15ms/step - loss: 6.1240e-04 - mse: 6.1240e-04 - mae: 0.0174 - val_loss: 3.1242e-04 - val_mse: 3.1242e-04 - val_mae: 0.0115\n",
            "Epoch 120/200\n",
            "29/29 [==============================] - 0s 15ms/step - loss: 5.9064e-04 - mse: 5.9064e-04 - mae: 0.0170 - val_loss: 3.3155e-04 - val_mse: 3.3155e-04 - val_mae: 0.0115\n",
            "Epoch 121/200\n",
            "29/29 [==============================] - 1s 18ms/step - loss: 5.7914e-04 - mse: 5.7914e-04 - mae: 0.0166 - val_loss: 3.0409e-04 - val_mse: 3.0409e-04 - val_mae: 0.0113\n",
            "Epoch 122/200\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 5.9167e-04 - mse: 5.9167e-04 - mae: 0.0169 - val_loss: 3.8772e-04 - val_mse: 3.8772e-04 - val_mae: 0.0126\n",
            "Epoch 123/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.4241e-04 - mse: 6.4241e-04 - mae: 0.0180 - val_loss: 3.0322e-04 - val_mse: 3.0322e-04 - val_mae: 0.0111\n",
            "Epoch 124/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.2781e-04 - mse: 6.2781e-04 - mae: 0.0179 - val_loss: 3.6307e-04 - val_mse: 3.6307e-04 - val_mae: 0.0137\n",
            "Epoch 125/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.0467e-04 - mse: 6.0467e-04 - mae: 0.0174 - val_loss: 3.3330e-04 - val_mse: 3.3330e-04 - val_mae: 0.0131\n",
            "Epoch 126/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.8027e-04 - mse: 5.8027e-04 - mae: 0.0165 - val_loss: 3.8572e-04 - val_mse: 3.8572e-04 - val_mae: 0.0127\n",
            "Epoch 127/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 6.2605e-04 - mse: 6.2605e-04 - mae: 0.0177 - val_loss: 3.4663e-04 - val_mse: 3.4663e-04 - val_mae: 0.0134\n",
            "Epoch 128/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.7855e-04 - mse: 5.7855e-04 - mae: 0.0167 - val_loss: 3.0041e-04 - val_mse: 3.0041e-04 - val_mae: 0.0111\n",
            "Epoch 129/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.6763e-04 - mse: 5.6763e-04 - mae: 0.0164 - val_loss: 3.2282e-04 - val_mse: 3.2282e-04 - val_mae: 0.0127\n",
            "Epoch 130/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.9804e-04 - mse: 5.9804e-04 - mae: 0.0175 - val_loss: 4.0847e-04 - val_mse: 4.0847e-04 - val_mae: 0.0132\n",
            "Epoch 131/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.8734e-04 - mse: 5.8734e-04 - mae: 0.0171 - val_loss: 2.9458e-04 - val_mse: 2.9458e-04 - val_mae: 0.0110\n",
            "Epoch 132/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.5609e-04 - mse: 5.5609e-04 - mae: 0.0162 - val_loss: 3.0720e-04 - val_mse: 3.0720e-04 - val_mae: 0.0120\n",
            "Epoch 133/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.5955e-04 - mse: 5.5955e-04 - mae: 0.0164 - val_loss: 2.9667e-04 - val_mse: 2.9667e-04 - val_mae: 0.0114\n",
            "Epoch 134/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 5.5559e-04 - mse: 5.5559e-04 - mae: 0.0162 - val_loss: 2.9258e-04 - val_mse: 2.9258e-04 - val_mae: 0.0113\n",
            "Epoch 135/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 5.5880e-04 - mse: 5.5880e-04 - mae: 0.0165 - val_loss: 3.1312e-04 - val_mse: 3.1312e-04 - val_mae: 0.0111\n",
            "Epoch 136/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.7511e-04 - mse: 5.7511e-04 - mae: 0.0167 - val_loss: 2.9046e-04 - val_mse: 2.9046e-04 - val_mae: 0.0114\n",
            "Epoch 137/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 5.4867e-04 - mse: 5.4867e-04 - mae: 0.0162 - val_loss: 2.8521e-04 - val_mse: 2.8521e-04 - val_mae: 0.0107\n",
            "Epoch 138/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 5.6580e-04 - mse: 5.6580e-04 - mae: 0.0164 - val_loss: 3.1306e-04 - val_mse: 3.1306e-04 - val_mae: 0.0111\n",
            "Epoch 139/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.5871e-04 - mse: 5.5871e-04 - mae: 0.0163 - val_loss: 2.8264e-04 - val_mse: 2.8264e-04 - val_mae: 0.0107\n",
            "Epoch 140/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 5.6095e-04 - mse: 5.6095e-04 - mae: 0.0164 - val_loss: 3.5608e-04 - val_mse: 3.5608e-04 - val_mae: 0.0140\n",
            "Epoch 141/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.5879e-04 - mse: 5.5879e-04 - mae: 0.0164 - val_loss: 2.8691e-04 - val_mse: 2.8691e-04 - val_mae: 0.0112\n",
            "Epoch 142/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.5670e-04 - mse: 5.5670e-04 - mae: 0.0164 - val_loss: 2.9460e-04 - val_mse: 2.9460e-04 - val_mae: 0.0108\n",
            "Epoch 143/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.4790e-04 - mse: 5.4790e-04 - mae: 0.0162 - val_loss: 3.5632e-04 - val_mse: 3.5632e-04 - val_mae: 0.0119\n",
            "Epoch 144/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.9154e-04 - mse: 5.9154e-04 - mae: 0.0174 - val_loss: 2.9397e-04 - val_mse: 2.9397e-04 - val_mae: 0.0116\n",
            "Epoch 145/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.8292e-04 - mse: 5.8292e-04 - mae: 0.0168 - val_loss: 3.5343e-04 - val_mse: 3.5343e-04 - val_mae: 0.0122\n",
            "Epoch 146/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 5.7155e-04 - mse: 5.7155e-04 - mae: 0.0167 - val_loss: 5.6892e-04 - val_mse: 5.6892e-04 - val_mae: 0.0171\n",
            "Epoch 147/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 6.8174e-04 - mse: 6.8174e-04 - mae: 0.0188 - val_loss: 2.8306e-04 - val_mse: 2.8306e-04 - val_mae: 0.0106\n",
            "Epoch 148/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.4923e-04 - mse: 5.4923e-04 - mae: 0.0164 - val_loss: 3.5575e-04 - val_mse: 3.5575e-04 - val_mae: 0.0145\n",
            "Epoch 149/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 5.5937e-04 - mse: 5.5937e-04 - mae: 0.0164 - val_loss: 2.9820e-04 - val_mse: 2.9820e-04 - val_mae: 0.0121\n",
            "Epoch 150/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.3467e-04 - mse: 5.3467e-04 - mae: 0.0158 - val_loss: 2.8131e-04 - val_mse: 2.8131e-04 - val_mae: 0.0112\n",
            "Epoch 151/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.3196e-04 - mse: 5.3196e-04 - mae: 0.0159 - val_loss: 2.8276e-04 - val_mse: 2.8276e-04 - val_mae: 0.0105\n",
            "Epoch 152/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.4274e-04 - mse: 5.4274e-04 - mae: 0.0161 - val_loss: 2.9485e-04 - val_mse: 2.9485e-04 - val_mae: 0.0120\n",
            "Epoch 153/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.2516e-04 - mse: 5.2516e-04 - mae: 0.0157 - val_loss: 2.7593e-04 - val_mse: 2.7593e-04 - val_mae: 0.0109\n",
            "Epoch 154/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 5.2116e-04 - mse: 5.2116e-04 - mae: 0.0156 - val_loss: 2.8634e-04 - val_mse: 2.8634e-04 - val_mae: 0.0116\n",
            "Epoch 155/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 5.2334e-04 - mse: 5.2334e-04 - mae: 0.0158 - val_loss: 2.7880e-04 - val_mse: 2.7880e-04 - val_mae: 0.0111\n",
            "Epoch 156/200\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 5.2693e-04 - mse: 5.2693e-04 - mae: 0.0160 - val_loss: 2.7684e-04 - val_mse: 2.7684e-04 - val_mae: 0.0111\n",
            "Epoch 157/200\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 5.1942e-04 - mse: 5.1942e-04 - mae: 0.0159 - val_loss: 2.7998e-04 - val_mse: 2.7998e-04 - val_mae: 0.0112\n",
            "Epoch 158/200\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 5.2261e-04 - mse: 5.2261e-04 - mae: 0.0158 - val_loss: 3.2436e-04 - val_mse: 3.2436e-04 - val_mae: 0.0114\n",
            "Epoch 159/200\n",
            "29/29 [==============================] - 1s 17ms/step - loss: 6.3651e-04 - mse: 6.3651e-04 - mae: 0.0182 - val_loss: 3.2152e-04 - val_mse: 3.2152e-04 - val_mae: 0.0115\n",
            "Epoch 160/200\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 5.2949e-04 - mse: 5.2949e-04 - mae: 0.0158 - val_loss: 2.8247e-04 - val_mse: 2.8247e-04 - val_mae: 0.0116\n",
            "Epoch 161/200\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 5.3770e-04 - mse: 5.3770e-04 - mae: 0.0163 - val_loss: 3.2859e-04 - val_mse: 3.2859e-04 - val_mae: 0.0137\n",
            "Epoch 162/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 5.3198e-04 - mse: 5.3198e-04 - mae: 0.0160 - val_loss: 2.9326e-04 - val_mse: 2.9326e-04 - val_mae: 0.0122\n",
            "Epoch 163/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 5.3712e-04 - mse: 5.3712e-04 - mae: 0.0163 - val_loss: 3.5641e-04 - val_mse: 3.5641e-04 - val_mae: 0.0146\n",
            "Epoch 164/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.1132e-04 - mse: 5.1132e-04 - mae: 0.0156 - val_loss: 2.8555e-04 - val_mse: 2.8555e-04 - val_mae: 0.0105\n",
            "Epoch 165/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.1351e-04 - mse: 5.1351e-04 - mae: 0.0155 - val_loss: 2.7037e-04 - val_mse: 2.7037e-04 - val_mae: 0.0109\n",
            "Epoch 166/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 4.9739e-04 - mse: 4.9739e-04 - mae: 0.0152 - val_loss: 2.6588e-04 - val_mse: 2.6588e-04 - val_mae: 0.0108\n",
            "Epoch 167/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.0601e-04 - mse: 5.0601e-04 - mae: 0.0157 - val_loss: 2.9947e-04 - val_mse: 2.9947e-04 - val_mae: 0.0107\n",
            "Epoch 168/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.3555e-04 - mse: 5.3555e-04 - mae: 0.0161 - val_loss: 2.8215e-04 - val_mse: 2.8215e-04 - val_mae: 0.0105\n",
            "Epoch 169/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.3415e-04 - mse: 5.3415e-04 - mae: 0.0161 - val_loss: 4.2613e-04 - val_mse: 4.2613e-04 - val_mae: 0.0165\n",
            "Epoch 170/200\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 5.8287e-04 - mse: 5.8287e-04 - mae: 0.0170 - val_loss: 2.6750e-04 - val_mse: 2.6750e-04 - val_mae: 0.0105\n",
            "Epoch 171/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 4.9244e-04 - mse: 4.9244e-04 - mae: 0.0152 - val_loss: 2.7223e-04 - val_mse: 2.7223e-04 - val_mae: 0.0113\n",
            "Epoch 172/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 4.9501e-04 - mse: 4.9501e-04 - mae: 0.0152 - val_loss: 2.7343e-04 - val_mse: 2.7343e-04 - val_mae: 0.0113\n",
            "Epoch 173/200\n",
            "29/29 [==============================] - 1s 18ms/step - loss: 5.0697e-04 - mse: 5.0697e-04 - mae: 0.0154 - val_loss: 2.6329e-04 - val_mse: 2.6329e-04 - val_mae: 0.0108\n",
            "Epoch 174/200\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 5.3942e-04 - mse: 5.3942e-04 - mae: 0.0164 - val_loss: 3.1017e-04 - val_mse: 3.1017e-04 - val_mae: 0.0109\n",
            "Epoch 175/200\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 5.0264e-04 - mse: 5.0264e-04 - mae: 0.0155 - val_loss: 2.6242e-04 - val_mse: 2.6242e-04 - val_mae: 0.0106\n",
            "Epoch 176/200\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 5.0018e-04 - mse: 5.0018e-04 - mae: 0.0154 - val_loss: 2.9978e-04 - val_mse: 2.9978e-04 - val_mae: 0.0127\n",
            "Epoch 177/200\n",
            "29/29 [==============================] - 1s 33ms/step - loss: 4.9038e-04 - mse: 4.9038e-04 - mae: 0.0151 - val_loss: 2.7137e-04 - val_mse: 2.7137e-04 - val_mae: 0.0114\n",
            "Epoch 178/200\n",
            "29/29 [==============================] - 1s 22ms/step - loss: 5.4358e-04 - mse: 5.4358e-04 - mae: 0.0164 - val_loss: 2.7860e-04 - val_mse: 2.7860e-04 - val_mae: 0.0117\n",
            "Epoch 179/200\n",
            "29/29 [==============================] - 1s 25ms/step - loss: 4.8335e-04 - mse: 4.8335e-04 - mae: 0.0149 - val_loss: 2.6146e-04 - val_mse: 2.6146e-04 - val_mae: 0.0108\n",
            "Epoch 180/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 4.9913e-04 - mse: 4.9913e-04 - mae: 0.0154 - val_loss: 3.3012e-04 - val_mse: 3.3012e-04 - val_mae: 0.0114\n",
            "Epoch 181/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.0287e-04 - mse: 5.0287e-04 - mae: 0.0155 - val_loss: 2.7955e-04 - val_mse: 2.7955e-04 - val_mae: 0.0103\n",
            "Epoch 182/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 4.9148e-04 - mse: 4.9148e-04 - mae: 0.0154 - val_loss: 2.8997e-04 - val_mse: 2.8997e-04 - val_mae: 0.0123\n",
            "Epoch 183/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.0657e-04 - mse: 5.0657e-04 - mae: 0.0156 - val_loss: 2.7187e-04 - val_mse: 2.7187e-04 - val_mae: 0.0114\n",
            "Epoch 184/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 4.9344e-04 - mse: 4.9344e-04 - mae: 0.0151 - val_loss: 2.6581e-04 - val_mse: 2.6581e-04 - val_mae: 0.0111\n",
            "Epoch 185/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 5.0933e-04 - mse: 5.0933e-04 - mae: 0.0158 - val_loss: 2.7582e-04 - val_mse: 2.7582e-04 - val_mae: 0.0102\n",
            "Epoch 186/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 4.8405e-04 - mse: 4.8405e-04 - mae: 0.0149 - val_loss: 2.6009e-04 - val_mse: 2.6009e-04 - val_mae: 0.0099\n",
            "Epoch 187/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 4.9944e-04 - mse: 4.9944e-04 - mae: 0.0157 - val_loss: 2.5281e-04 - val_mse: 2.5281e-04 - val_mae: 0.0101\n",
            "Epoch 188/200\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 4.8335e-04 - mse: 4.8335e-04 - mae: 0.0151 - val_loss: 2.5876e-04 - val_mse: 2.5876e-04 - val_mae: 0.0102\n",
            "Epoch 189/200\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 4.8223e-04 - mse: 4.8223e-04 - mae: 0.0150 - val_loss: 2.4857e-04 - val_mse: 2.4857e-04 - val_mae: 0.0100\n",
            "Epoch 190/200\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 4.8433e-04 - mse: 4.8433e-04 - mae: 0.0150 - val_loss: 2.6099e-04 - val_mse: 2.6099e-04 - val_mae: 0.0110\n",
            "Epoch 191/200\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 4.9759e-04 - mse: 4.9759e-04 - mae: 0.0153 - val_loss: 2.8947e-04 - val_mse: 2.8947e-04 - val_mae: 0.0124\n",
            "Epoch 192/200\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 4.7777e-04 - mse: 4.7777e-04 - mae: 0.0149 - val_loss: 3.0607e-04 - val_mse: 3.0607e-04 - val_mae: 0.0109\n",
            "Epoch 193/200\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 4.9240e-04 - mse: 4.9240e-04 - mae: 0.0154 - val_loss: 2.5925e-04 - val_mse: 2.5925e-04 - val_mae: 0.0109\n",
            "Epoch 194/200\n",
            "29/29 [==============================] - 0s 15ms/step - loss: 4.8207e-04 - mse: 4.8207e-04 - mae: 0.0151 - val_loss: 2.6411e-04 - val_mse: 2.6411e-04 - val_mae: 0.0112\n",
            "Epoch 195/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 4.9085e-04 - mse: 4.9085e-04 - mae: 0.0151 - val_loss: 2.5458e-04 - val_mse: 2.5458e-04 - val_mae: 0.0102\n",
            "Epoch 196/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 4.7107e-04 - mse: 4.7107e-04 - mae: 0.0147 - val_loss: 2.6851e-04 - val_mse: 2.6851e-04 - val_mae: 0.0102\n",
            "Epoch 197/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 4.8888e-04 - mse: 4.8888e-04 - mae: 0.0152 - val_loss: 2.4841e-04 - val_mse: 2.4841e-04 - val_mae: 0.0102\n",
            "Epoch 198/200\n",
            "29/29 [==============================] - 0s 9ms/step - loss: 4.7711e-04 - mse: 4.7711e-04 - mae: 0.0151 - val_loss: 2.4621e-04 - val_mse: 2.4621e-04 - val_mae: 0.0101\n",
            "Epoch 199/200\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 5.0519e-04 - mse: 5.0519e-04 - mae: 0.0154 - val_loss: 2.7396e-04 - val_mse: 2.7396e-04 - val_mae: 0.0117\n",
            "Epoch 200/200\n",
            "29/29 [==============================] - 0s 10ms/step - loss: 4.7040e-04 - mse: 4.7040e-04 - mae: 0.0147 - val_loss: 2.8722e-04 - val_mse: 2.8722e-04 - val_mae: 0.0122\n",
            "29/29 [==============================] - 0s 3ms/step\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "Training RMSE: 1517.1153013026342\n",
            "Training R2 score: 0.9924603852573329\n",
            "Testing RMSE: 1154.344799826841\n",
            "Testing R2 score: 0.9931848304001972\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'2024-04-22': 65474.1171875,\n",
              " '2024-04-23': 65901.6796875,\n",
              " '2024-04-24': 66316.78125,\n",
              " '2024-04-25': 66801.015625,\n",
              " '2024-04-26': 67340.3046875,\n",
              " '2024-04-27': 67905.0,\n",
              " '2024-04-28': 68433.8984375,\n",
              " '2024-04-29': 68965.8203125,\n",
              " '2024-04-30': 69575.828125,\n",
              " '2024-05-01': 70220.328125,\n",
              " '2024-05-02': 70899.75,\n",
              " '2024-05-03': 71563.828125,\n",
              " '2024-05-04': 72249.265625,\n",
              " '2024-05-05': 72948.7890625,\n",
              " '2024-05-06': 73676.7734375,\n",
              " '2024-05-07': 74429.328125,\n",
              " '2024-05-08': 75208.640625,\n",
              " '2024-05-09': 76006.484375,\n",
              " '2024-05-10': 76820.6640625,\n",
              " '2024-05-11': 77656.5,\n",
              " '2024-05-12': 78518.6953125,\n",
              " '2024-05-13': 79410.6796875,\n",
              " '2024-05-14': 80334.171875,\n",
              " '2024-05-15': 81294.34375,\n",
              " '2024-05-16': 82298.8046875,\n",
              " '2024-05-17': 83349.9765625,\n",
              " '2024-05-18': 84451.828125,\n",
              " '2024-05-19': 85608.46875,\n",
              " '2024-05-20': 86824.421875,\n",
              " '2024-05-21': 88104.6875,\n",
              " '2024-05-22': 89465.0859375,\n",
              " '2024-05-23': 90918.25,\n",
              " '2024-05-24': 92478.3203125,\n",
              " '2024-05-25': 94154.1875,\n",
              " '2024-05-26': 95970.2578125,\n",
              " '2024-05-27': 97942.3515625,\n",
              " '2024-05-28': 100085.4609375,\n",
              " '2024-05-29': 102436.265625,\n",
              " '2024-05-30': 105024.09375,\n",
              " '2024-05-31': 107881.5859375,\n",
              " '2024-06-01': 111056.1015625,\n",
              " '2024-06-02': 114618.4453125,\n",
              " '2024-06-03': 118635.3046875,\n",
              " '2024-06-04': 123190.4609375,\n",
              " '2024-06-05': 128397.109375,\n",
              " '2024-06-06': 134397.53125,\n",
              " '2024-06-07': 141372.21875,\n",
              " '2024-06-08': 149558.890625,\n",
              " '2024-06-09': 159274.734375,\n",
              " '2024-06-10': 170943.59375,\n",
              " '2024-06-11': 185149.3125,\n",
              " '2024-06-12': 202736.9375,\n",
              " '2024-06-13': 224963.4375,\n",
              " '2024-06-14': 253627.265625,\n",
              " '2024-06-15': 291396.0,\n",
              " '2024-06-16': 342312.4375,\n",
              " '2024-06-17': 412639.125,\n",
              " '2024-06-18': 512558.03125,\n",
              " '2024-06-19': 660103.75,\n",
              " '2024-06-20': 891846.125}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}